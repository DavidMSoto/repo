
OutofMilton
{
	/*
	no se te ha podido ocurrir un metodo mas complicado para marcharte de miton ?
	vender humo en el mundo de los BA, y marcharte a una zona donde sintetizar la vitamina D no sea tan complicado, ivan te muestra el camino.
	
	*/
	
}
howto{
/*
	How can a beginner train for machine learning contests?

As a long-time programmer I think you already have a huge advantage.

Competitive machine learning (which should actually be called for what it is: competitive modelling) is all about fail-forward-fast. 
Try many things and see what works. And being a proficient programmer will obviously be a huge advantage because it will allow you to code fast 
and iterate through hypotheses quickly.

Now, I’m going off the assumption that you meant what you said, which is that you want to become good a competitive machine learning,
 which requires an entirely different strategy than what you’d do if you wanted to become good at data science or machine learning in general.

The map Mike Scarpati provided is a really good representation of the landscape in data science. It can be intimidating, but fortunately for you you’ll 
need to conquer only a small section of that map to become good at ML competitions.

One thing I want to challenge is your one-year plan. No need to train for a year, just start now, understand that you’ll be mediocre at the beginning
 and will improve throughout the year. That’s what I did.

In order to excel you’ll need to master concepts like ensembling and metamodelling, you’ll need to learn tricks like “fitting the leaderboard”
 and semi-supervised learning, but that should be part of your second half of the year. 
 The first 6 months you need to focus on one thing: cross validation. 
 It will not be hard to throw a few models at a problem, just follow some tutorials or code posted on Kaggle forums and you’ll be up and running quickly.
  It doesn’t even matter if you understand the models at the beginning. But you have got to make sure you understand cross validation well, 
  learn when to trust it and how to use it. Unless you do that, you won’t be able to feel confident about your progress.

Once you and cross validation are best friends, start diving into the different algorithms based on how much you are comfortable with. 
There is no pre-determined amount of knowledge you should have on the math. Just learn what it’s enough for you. 
Do not force yourself to learn too much, or you’ll hate it. Some people enjoy diving into the math, if that’s your case, do it. 
Other, like me, thrive on understanding the algorithms “practically” and learning just enough of the underlying math to support the “practical” aspect.

Take cross validation, a few algorithms, and start throwing those at problems, measure progress through the leaderboard and keep failing forward fast.
 Everything else will fall into place naturally.
*/
	
}

class formacion ()
{
map Mike Scarpati
https://www.coursera.org/learn/machine-learning
}


class CrossValidation{
/*
*  deja de preocuparte por la imagen que dejas en kaggle, 
*  - abrete una cuenta nueva, usa un modelo que ya existe e intenta cros validarlo, 
* 	como se hace eso ?
*/
http://www.milanor.net/blog/cross-validation-for-predictive-analytics-using-r/

}
class modelling {
Metrics {
// antes de comenzar a el analisis de datos lo primero es entender el enunciado del problema, porque la solucion esta escrita en su mismo lenguaje. 


// las competiciones se evuluan en base a la metrica definida  ie - Logarithmic Loss
https://www.kaggle.com/wiki/Metrics
}

Algoritmos (approach){
las herramientas para resolver los problemas son los  Algoritmos  
http://jmlr.org/papers/volume15/delgado14a/delgado14a.pdf
https://www.kaggle.com/wiki/Algorithms

}

Binomios (Metricas, Algoritmos)
{
//Random Forest parece ser que es la que mas peta 

	Multi-Class Classification: Forest Cover Type Prediction
	https://www.kaggle.com/c/forest-cover-type-prediction
	
	Binary Classification: Titanic: Machine Learning from Disaster
	https://www.kaggle.com/c/titanic
	
	Regression with temporal component: Bike Sharing Demand
	https://www.kaggle.com/c/bike-sharing-demand
	
	Binary Classification with text data: Random Acts of Pizza
	https://www.kaggle.com/c/random-acts-of-pizza
}
}

class competiciones()
{
	
#Sun 31 Jul 2016 ( w29 / w30 )
Submissions are evaluated using the multi-class logarithmic loss
-- Shelter -- just submision 
idea mix up and inteligence



#Tue 30 Aug 2016 - (w31/w34) Bimbo **
The evaluation metric for this competition is Root Mean Squared Logarithmic Error.
	
}

class tecnologias ()
{

python ()
{
ipython notebook --ip=0.0.0.0 --port=8889

}
R()
{
https://www.kaggle.com/c/titanic/details/new-getting-started-with-r
https://github.com/wehrley/wehrley.github.io/blob/master/SOUPTONUTS.md

}

}



